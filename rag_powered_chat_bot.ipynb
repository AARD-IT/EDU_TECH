{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a9c01-60ab-4e7a-bfb4-c76c55423934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self):\n",
    "        self.embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        self.vector_db_path = \"pdf_vector_db\"\n",
    "        os.makedirs(self.vector_db_path, exist_ok=True)\n",
    "\n",
    "        # OpenRouter API setup\n",
    "        os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "        os.environ[\"OPENAI_BASE_URL\"] = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        \"\"\"Process PDF and create/update vector DB\"\"\"\n",
    "        db_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        vector_path = os.path.join(self.vector_db_path, db_name)\n",
    "\n",
    "        flag_path = os.path.join(vector_path, \"processed.flag\")\n",
    "        if os.path.exists(flag_path):\n",
    "            print(f\"[SKIP] Already processed: {pdf_path}\")\n",
    "            return vector_path\n",
    "\n",
    "        print(f\"[PROCESSING] Creating vector DB for: {pdf_path}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "\n",
    "            text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "            vectordb = Chroma.from_documents(\n",
    "                documents=chunks,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=vector_path\n",
    "            )\n",
    "            vectordb.persist()\n",
    "\n",
    "            with open(flag_path, \"w\") as f:\n",
    "                f.write(\"processed\")\n",
    "\n",
    "            print(f\"[SUCCESS] Vector DB created at: {vector_path}\")\n",
    "            return vector_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {str(e)}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if 'documents' in locals(): del documents\n",
    "            if 'chunks' in locals(): del chunks\n",
    "            if 'vectordb' in locals(): del vectordb\n",
    "            gc.collect()\n",
    "\n",
    "    def query_document(self, vector_path, question):\n",
    "        \"\"\"Retrieve context & generate answer using OpenRouter\"\"\"\n",
    "        if not os.path.exists(vector_path):\n",
    "            raise ValueError(\"Vector DB not found at specified path\")\n",
    "\n",
    "        vectordb = Chroma(\n",
    "            persist_directory=vector_path,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "\n",
    "        # Retrieve top 3 relevant chunks\n",
    "        results = vectordb.similarity_search(question, k=3)\n",
    "        if not results:\n",
    "            return \"No relevant information found.\"\n",
    "\n",
    "        context = \" \".join([r.page_content for r in results])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are a helpful assistant. Use the following context to answer:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"mistralai/mixtral-8x7b-instruct\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=250\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = PDFProcessor()\n",
    "\n",
    "    pdf_path = r\"C:\\Users\\User\\Downloads\\admini_details_new.pdf\"\n",
    "    vector_path = processor.process_pdf(pdf_path)\n",
    "\n",
    "    if vector_path:\n",
    "        while True:\n",
    "            question = input(\"\\nAsk a question (or 'quit' to exit): \")\n",
    "            if question.lower() == \"quit\":\n",
    "                break\n",
    "quit\n",
    "            answer = processor.query_document(vector_path, question)\n",
    "            print(f\"\\nAnswer:\\n{answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
